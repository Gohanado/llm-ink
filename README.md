# LLM-Ink

Infrastructure complÃ¨te pour dÃ©ployer un serveur LLM local (CPU/GPU) avec [llama.cpp](https://github.com/ggerganov/llama.cpp), Docker, et API HTTP (port 5000).

## ğŸ”§ Stack utilisÃ©e

- ğŸ‹ Docker / Docker Compose
- ğŸ§  llama.cpp (compilÃ© avec CMake)
- ğŸ“¦ Mistral 7B Instruct GGUF (modÃ¨le par dÃ©faut)
- ğŸ›¡ï¸ Volume montÃ© sur `/mnt/volume-data` pour la persistance
- âš™ï¸ Portainer pour la gestion visuelle de Docker

## ğŸš€ DÃ©marrage rapide

```bash
cd llm-server
docker compose up -d --build
```

## ğŸ“ Arborescence

- `llm-server/` : Dockerfile + entrypoint.sh pour le serveur LLM
- `infra/` : Docker Compose pour Portainer
- `DOCUMENTATION.md` : historique technique du projet, Ã©tapes par Ã©tapes

## ğŸ” AccÃ¨s

- Portainer : http://<ip-vps>:9000
- API LLM : http://<ip-vps>:5000
